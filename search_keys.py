import os
import requests
import time
import random
import string
from datetime import datetime

# --- é…ç½®åŒº ---
GITHUB_TOKEN = os.getenv('GH_PAT')
if not GITHUB_TOKEN:
    raise ValueError("æœªæ‰¾åˆ° GitHub PATã€‚è¯·è®¾ç½® 'GH_PAT' ç¯å¢ƒå˜é‡ã€‚")

# åŸºç¡€æœç´¢å‰ç¼€
BASE_QUERY_PREFIX = 'AIzaSy'

# ä¿å­˜ç»“æœçš„æ–‡ä»¶å
OUTPUT_FILE = "api.txt"

# GitHub API è¯·æ±‚å¤´
HEADERS = {
    'Authorization': f'token {GITHUB_TOKEN}',
    'Accept': 'application/vnd.github.v3.text-match+json'
}

# --- åŠŸèƒ½å‡½æ•° ---

def generate_search_queries(prefix):
    """
    ã€æ ¸å¿ƒå¢å¼ºã€‘ç”Ÿæˆæ›´ç²¾ç»†çš„æœç´¢æŸ¥è¯¢åˆ—è¡¨ã€‚
    å°† 'AIzaSy' æ‰©å±•ä¸º ['AIzaSyA', 'AIzaSyB', ..., 'AIzaSy9', 'AIzaSy-', 'AIzaSy_']
    """
    # åŒ…å«æ‰€æœ‰å­—æ¯ï¼ˆå¤§å°å†™ï¼‰ã€æ•°å­—ï¼Œä»¥åŠAPI Keyä¸­å¸¸è§çš„'-'å’Œ'_'
    characters_to_try = string.ascii_letters + string.digits + '-_'
    queries = [f'"{prefix}{char}"' for char in characters_to_try]
    print(f"å·²ç”Ÿæˆ {len(queries)} ä¸ªç²¾ç»†åŒ–æœç´¢æŸ¥è¯¢ï¼Œä¾‹å¦‚ï¼š{queries[0]}, {queries[10]}, {queries[-1]}")
    return queries

def check_rate_limit():
    """æ£€æŸ¥GitHub APIçš„é€Ÿç‡é™åˆ¶ï¼Œå¦‚æœæ¥è¿‘é™åˆ¶åˆ™æš‚åœç­‰å¾…ã€‚"""
    try:
        response = requests.get('https://api.github.com/rate_limit', headers=HEADERS)
        response.raise_for_status()
        rate_info = response.json().get('resources', {}).get('search', {})
        remaining = rate_info.get('remaining', 0)
        reset_time = rate_info.get('reset', 0)
        
        print(f"APIé€Ÿç‡é™åˆ¶: å‰©ä½™ {remaining} æ¬¡è¯·æ±‚ã€‚é‡ç½®æ—¶é—´: {datetime.fromtimestamp(reset_time).strftime('%Y-%m-%d %H:%M:%S')}")
        
        if remaining < 5:
            sleep_time = max(0, reset_time - time.time()) + 5
            print(f"é€Ÿç‡é™åˆ¶è¿‡ä½ï¼Œç¨‹åºå°†æš‚åœ {sleep_time:.2f} ç§’ã€‚")
            time.sleep(sleep_time)
            
    except requests.exceptions.RequestException as e:
        print(f"æ£€æŸ¥é€Ÿç‡é™åˆ¶æ—¶å‡ºé”™: {e}")
        time.sleep(60)

def search_github(query):
    """æ ¹æ®å•ä¸ªç²¾ç»†åŒ–çš„æŸ¥è¯¢å­—ç¬¦ä¸²åœ¨ GitHub ä¸Šæœç´¢ä»£ç ã€‚"""
    found_keys = set()
    base_url = 'https://api.github.com/search/code'
    params = {'q': query, 'per_page': 100, 'page': 1}
    
    print(f"ğŸš€ å¼€å§‹ç²¾ç»†æœç´¢: {query}")
    
    page_count = 0
    while True:
        # GitHub APIé™åˆ¶æ¯ä¸ªæŸ¥è¯¢æœ€å¤šåªèƒ½è®¿é—®10é¡µï¼ˆ1000ä¸ªç»“æœï¼‰
        if page_count >= 10:
            print(f"å·²è¾¾åˆ°æŸ¥è¯¢ '{query}' çš„10é¡µï¼ˆ1000ä¸ªç»“æœï¼‰ä¸Šé™ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªæŸ¥è¯¢ã€‚")
            break

        check_rate_limit()
        try:
            response = requests.get(base_url, headers=HEADERS, params=params)
            response.raise_for_status()
            data = response.json()
        except requests.exceptions.HTTPError as e:
            if e.response.status_code in [422, 403]:
                print(f"æŸ¥è¯¢ '{query}' é‡åˆ°APIé™åˆ¶æˆ–æ— æ•ˆæŸ¥è¯¢ï¼Œè·³è¿‡ã€‚é”™è¯¯: {e}")
                break
            print(f"HTTPè¯·æ±‚é”™è¯¯: {e}")
            time.sleep(60)
            continue
        except requests.exceptions.RequestException as e:
            print(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
            time.sleep(60)
            continue
            
        items = data.get('items', [])
        if not items:
            print(f"æŸ¥è¯¢ '{query}' åœ¨ç¬¬ {params['page']} é¡µæ— ç»“æœï¼Œç»“æŸæ­¤æŸ¥è¯¢ã€‚")
            break

        for item in items:
            repo_name = item.get('repository', {}).get('full_name', 'æœªçŸ¥ä»“åº“')
            file_path = item.get('path', 'æœªçŸ¥è·¯å¾„')
            
            for match in item.get('text_matches', []):
                line_content = match.get('fragment', '')
                words = line_content.replace(':', ' ').replace('=', ' ').split() # åˆ†å‰²æ›´å¤šå¯èƒ½çš„åˆ†éš”ç¬¦
                for word in words:
                    cleaned_word = word.strip('\'",;()[]{}<>`')
                    # æˆ‘ä»¬æŸ¥è¯¢çš„æ˜¯ "AIzaSyA"ï¼Œæ‰€ä»¥è¦ç¡®ä¿æ‰¾åˆ°çš„è¯ä»¥å®ƒå¼€å¤´
                    if cleaned_word.startswith(query.strip('"')) and len(cleaned_word) > len(BASE_QUERY_PREFIX):
                        # print(f"  [+] å‘ç°æ½œåœ¨Key: {cleaned_word} | æ¥æº: {repo_name}/{file_path}")
                        found_keys.add(cleaned_word)

        if 'next' in response.links:
            params['page'] += 1
            page_count += 1
            time.sleep(random.uniform(2, 4)) # éšæœºæš‚åœï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        else:
            print(f"âœ… æŸ¥è¯¢ '{query}' å®Œæˆã€‚")
            break
            
    return found_keys

def main():
    """ä¸»å‡½æ•°ï¼Œè´Ÿè´£æ•´ä¸ªæµç¨‹çš„è°ƒåº¦ã€‚"""
    all_found_keys = set()

    # è¯»å– api.txt ä¸­å·²æœ‰çš„Keyï¼Œå¹¶åœ¨è¯»å–æ—¶è¿›è¡Œæ¸…æ´—
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                clean_key = line.split('|')[0].strip()
                if clean_key:
                    all_found_keys.add(clean_key)
        print(f"å·²ä» {OUTPUT_FILE} åŠ è½½å¹¶æ¸…æ´—äº† {len(all_found_keys)} æ¡å·²æœ‰è®°å½•ã€‚")

    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ç”Ÿæˆç²¾ç»†åŒ–çš„æŸ¥è¯¢åˆ—è¡¨
    SEARCH_QUERIES = generate_search_queries(BASE_QUERY_PREFIX)

    # éå†æ‰€æœ‰ç²¾ç»†åŒ–çš„æŸ¥è¯¢æ¡ä»¶
    for i, query in enumerate(SEARCH_QUERIES):
        print(f"\n--- æ­£åœ¨æ‰§è¡Œç¬¬ {i+1}/{len(SEARCH_QUERIES)} ä¸ªä¸»æŸ¥è¯¢ç³»åˆ— ---")
        keys_from_query = search_github(query)
        new_keys_count = len(keys_from_query - all_found_keys)
        if new_keys_count > 0:
            print(f"ğŸ‰ æŸ¥è¯¢ '{query}' å‘ç° {new_keys_count} ä¸ªæ–°Keyï¼")
            all_found_keys.update(keys_from_query)
        else:
            print(f"æŸ¥è¯¢ '{query}' æœªå‘ç°æ–°Keyã€‚")
    
    print(f"\næœç´¢å®Œæˆã€‚å…±è®¡ {len(all_found_keys)} æ¡ä¸é‡å¤è®°å½•ï¼Œå°†å†™å…¥ {OUTPUT_FILE}...")

    # å°†æ‰€æœ‰ä¸é‡å¤çš„Keyæ’åºåå†™å…¥æ–‡ä»¶
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for key in sorted(list(all_found_keys)):
            f.write(f"{key}\n")
    
    print("âœ¨ ä»»åŠ¡å®Œæˆï¼")

if __name__ == '__main__':
    main()
